{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start zookeeper\n",
    "# ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties\n",
    "\n",
    "# start kafka\n",
    "# ./bin/kafka-server-start ./etc/kafka/server.properties\n",
    "\n",
    "# List topics\n",
    "#./bin/kafka-topics --zookeeper localhost:2181 --list\n",
    "\n",
    "# Delete a topic\n",
    "#./bin/kafka-topics --zookeeper localhost:2181 --delete --topic write-to-neo4j\n",
    "\n",
    "#https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n",
    "\n",
    "# start neo4j\n",
    "# ./bin/neo4j restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Demo Neo4j Graph, with Neo4j BOLT Protocol Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'properties_set': 150000}\n",
      "{'constraints_added': 1}\n",
      "{'constraints_added': 1}\n",
      "{'indexes_added': 1}\n",
      "{'relationships_created': 150000, 'nodes_created': 150000, 'labels_added': 150000, 'properties_set': 150000}\n",
      "*** Done!\n"
     ]
    }
   ],
   "source": [
    "#make sure apoc procedures are installed in Neo4j plugins folder\n",
    "\n",
    "from neo4j.v1 import GraphDatabase, basic_auth, TRUST_ON_FIRST_USE, CypherError\n",
    "from string import Template\n",
    "\n",
    "\n",
    "nodes = 150000\n",
    "\n",
    "nodes_per_graph = 5000\n",
    "\n",
    "graphs = int(nodes/nodes_per_graph)\n",
    "\n",
    "query0 = 'MATCH (n) DETACH DELETE n'\n",
    "\n",
    "\n",
    "query1 = Template('CALL apoc.generate.ba( ${nodes_per_graph}, 1, \"Cinema\", \"HAS_LOCATION\") '\n",
    ").substitute(locals())\n",
    "\n",
    "\n",
    "query2 = '''\n",
    "MATCH (c:Cinema) SET c.cinemaId = id(c)+1000000\n",
    ";\n",
    "'''\n",
    "query3 = '''\n",
    "CREATE CONSTRAINT ON (c:Cinema) ASSERT c.cinemaId IS UNIQUE\n",
    ";\n",
    "'''\n",
    "query4 = '''\n",
    "CREATE CONSTRAINT ON (a:Account) ASSERT a.accountId IS UNIQUE\n",
    ";\n",
    "'''\n",
    "\n",
    "query5 = '''\n",
    "CREATE INDEX on :DailyBoxOffice(accountId)\n",
    ";    \n",
    "'''\n",
    "\n",
    "query6 = '''\n",
    "MATCH (c:Cinema) \n",
    "WHERE NOT EXISTS ( (c)-[:HAS_ACCOUNT]->() )\n",
    "CREATE (a:Account)<-[:HAS_ACCOUNT]-(c) SET a.accountId = c.cinemaId\n",
    ";\n",
    "'''\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost\",\n",
    "                          auth=basic_auth(\"neo4j\", \"neo4j\"),\n",
    "                          encrypted=False,\n",
    "                          trust=TRUST_ON_FIRST_USE)\n",
    "try:\n",
    "    \n",
    "    session = driver.session()\n",
    "    result = session.run(query0)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "                 \n",
    "    session = driver.session()\n",
    "    for i in range(graphs):\n",
    "        result = session.run(query1)\n",
    "        summary = result.consume()\n",
    "        #print(summary.counters)\n",
    "    session.close()\n",
    "    \n",
    "    session = driver.session()\n",
    "    result = session.run(query2)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "    \n",
    "    session = driver.session()\n",
    "    result = session.run(query3)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "    \n",
    "    session = driver.session()\n",
    "    result = session.run(query4)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "    \n",
    "    session = driver.session()\n",
    "    result = session.run(query5)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "    \n",
    "    session = driver.session()\n",
    "    result = session.run(query6)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "\n",
    "    print('*** Got exception',e)\n",
    "    if not isinstance(e, CypherError):\n",
    "        print('*** Rolling back')\n",
    "        session.rollback()\n",
    "    else:\n",
    "        print('*** Not rolling back')\n",
    "\n",
    "finally:        \n",
    "     print('*** Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "### Set Topic, Configure Messages, Timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example message: b'1000000,9611.06,1483655125.567612'\n",
      "Message size (bytes): 33\n"
     ]
    }
   ],
   "source": [
    "# Initializations.\n",
    "import random\n",
    "import time\n",
    "\n",
    "# connect to Kafka\n",
    "bootstrap_servers = 'localhost:9092' # change if your brokers live else where\n",
    "\n",
    "kafka_topic = 'neo4j-150K-demo2'\n",
    "\n",
    "msg_count = 150000\n",
    "\n",
    "# this is the total number of messages that will be generated\n",
    "\n",
    "# function to generate messages that will be the data for the graph update\n",
    "\n",
    "# an example message is displayed : accountId, revenue, timestamp\n",
    "# this simulates data from the source database\n",
    "\n",
    "i=0\n",
    "def generate_message(i):\n",
    "    msg_payload = (str(i+1000000) + ',' + str(random.randrange(100000,1000000)/100) + ',' + str(time.time())).encode()\n",
    "    return(msg_payload)\n",
    "\n",
    "example_message = generate_message(i)\n",
    "msg_bytes = len(generate_message(i))\n",
    "\n",
    "print(\"Example message: \" + str(example_message))\n",
    "print(\"Message size (bytes): \" + str(msg_bytes))\n",
    "\n",
    "\n",
    "# we'll use a timer so you can see the throughput for both\n",
    "# the producer and the consumer\n",
    "\n",
    "# reset timer for kafka producer and consumer\n",
    "\n",
    "producer_timings = {}\n",
    "consumer_timings = {}\n",
    "\n",
    "\n",
    "\n",
    "# function to calc throughput based on msg count and length\n",
    "\n",
    "def calculate_thoughput(timing, n_messages=msg_count, msg_size=msg_bytes):\n",
    "    print(\"Processed {0} messsages in {1:.2f} seconds\".format(n_messages, timing))\n",
    "    print(\"{0:.2f} MB/s\".format((msg_size * n_messages) / timing / (1024*1024)))\n",
    "    print(\"{0:.2f} Msgs/s\".format(n_messages / timing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Message Producer using Confluent_Kafka Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kafka producer function, simulates ETL data stream for graph updates\n",
    "\n",
    "from confluent_kafka import Producer, KafkaException, KafkaError\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "topic = kafka_topic\n",
    "\n",
    "def confluent_kafka_producer_performance():\n",
    "    \n",
    "    # Note that you need to set producer buffer to at least as large as number of messages\n",
    "    # otherwise you'll get a buffer overflow and the sequential messages will be corrupted\n",
    "    conf = {'bootstrap.servers': bootstrap_servers, \n",
    "            'queue.buffering.max.messages': 200000\n",
    "    }\n",
    "    \n",
    "    producer = confluent_kafka.Producer(**conf)\n",
    "    i = 0\n",
    "    messages_overflow = 0\n",
    "    producer_start = time.time()\n",
    "    for i in range(msg_count):\n",
    "        msg_payload = generate_message(i)\n",
    "        try:\n",
    "            producer.produce(topic, value=msg_payload) \n",
    "        except BufferError as e:\n",
    "            messages_overflow += 1\n",
    "\n",
    "    # checking for overflow\n",
    "    print('BufferErrors: ' + str(messages_overflow))\n",
    "\n",
    "    producer.flush()\n",
    "            \n",
    "    return time.time() - producer_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BufferErrors: 0\n",
      "Processed 150000 messsages in 1.56 seconds\n",
      "3.03 MB/s\n",
      "96150.94 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "producer_timings['confluent_kafka_producer'] = confluent_kafka_producer_performance()\n",
    "calculate_thoughput(producer_timings['confluent_kafka_producer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Produced Messages by Inspecting Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: OffsetPartitionResponse(offset=[0], err=0)}\n",
      "{0: OffsetPartitionResponse(offset=[150000], err=0)}\n"
     ]
    }
   ],
   "source": [
    "from pykafka import KafkaClient\n",
    "\n",
    "client = KafkaClient(hosts=bootstrap_servers)\n",
    "topic = client.topics[kafka_topic.encode()]\n",
    "print(topic.earliest_available_offsets())\n",
    "print(topic.latest_available_offsets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Message Consumer using Confluent_Kafka, with Neo4j BOLT Protocol Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import confluent_kafka\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "import sys\n",
    "import getopt\n",
    "import json\n",
    "from pprint import pformat\n",
    "import uuid\n",
    "from neo4j.v1 import GraphDatabase, basic_auth, TRUST_ON_FIRST_USE, CypherError\n",
    "#import pandas as pd  #uncomment if you want to write messages to a file\n",
    "\n",
    "\n",
    "\n",
    "def confluent_kafka_consume_batch(consumer, batch_size):\n",
    "\n",
    "            batch_list = []\n",
    "            \n",
    "            batch_msg_consumed = 0\n",
    "\n",
    "            for m in range(batch_size):\n",
    "\n",
    "                msg = consumer.poll()\n",
    "\n",
    "                if msg is None:\n",
    "                    break\n",
    "                    #continue\n",
    "\n",
    "                if msg.error():\n",
    "                    # Error or event\n",
    "                    if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                        # End of partition event\n",
    "                        sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                         (msg.topic(), msg.partition(), msg.offset()))\n",
    "                    elif msg.error():\n",
    "                        # Error\n",
    "                        raise KafkaException(msg.error())  \n",
    "                        \n",
    "                else:\n",
    "\n",
    "                    datastr = str(msg.value())\n",
    "                    data = datastr[2:-1].split(\",\")\n",
    "                    \n",
    "                    # details you can access from message object\n",
    "                    # print(\"%s %s\" % (\"iterator:\", m))\n",
    "                    # print(\"%s %s\" % (\"msg:\", str(msg.value())))\n",
    "                    # print(\"%s %s\" % (\"length:\", len(msg)))\n",
    "                    # print(\"%s %s\" % (\"data:\", data))\n",
    "\n",
    "                    batch_list.extend([data])\n",
    "                    \n",
    "                    batch_msg_consumed += 1\n",
    "                        \n",
    "            return(batch_list, batch_msg_consumed)\n",
    "\n",
    "        \n",
    "\n",
    "def confluent_kafka_consumer_performance():\n",
    "    \n",
    "    topic = kafka_topic\n",
    "    msg_consumed_count = 0\n",
    "    batch_size = 10000\n",
    "    batch_list = []\n",
    "    nodes = 0\n",
    "    rels = 0 \n",
    "    \n",
    "    driver = GraphDatabase.driver(\"bolt://localhost\",\n",
    "                              auth=basic_auth(\"neo4j\", \"neo4j\"),\n",
    "                              encrypted=False,\n",
    "                              trust=TRUST_ON_FIRST_USE)\n",
    "    \n",
    "    \n",
    "    update_query = '''\n",
    "    WITH  {batch_list} AS batch_list\n",
    "    UNWIND batch_list AS rows\n",
    "    WITH rows, toInteger(rows[0]) AS acctid\n",
    "    MATCH (a:Account {accountId: acctid}) \n",
    "    MERGE (a)-[r:HAS_DAILY_REVENUE]->(n:DailyBoxOffice {accountId: toInteger(rows[0])})\n",
    "    ON CREATE SET n.revenueUSD = toFloat(rows[1]), n.createdDate = toFloat(rows[2])\n",
    "    '''\n",
    "    \n",
    "    conf = {'bootstrap.servers': bootstrap_servers,\n",
    "            'group.id': uuid.uuid1(),\n",
    "            'session.timeout.ms': 60000,\n",
    "            'enable.auto.commit': 'true',\n",
    "            'default.topic.config': {\n",
    "                'auto.offset.reset': 'earliest'\n",
    "            }\n",
    "    }\n",
    "\n",
    "    consumer = confluent_kafka.Consumer(**conf)\n",
    "\n",
    "    consumer_start = time.time()\n",
    "    \n",
    "    def print_assignment (consumer, partitions):\n",
    "        print('Assignment:', partitions)\n",
    "    \n",
    "    # Subscribe to topics\n",
    "    consumer.subscribe([topic], on_assign=print_assignment)\n",
    "   \n",
    "    # consumer loop\n",
    "    try:\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # Neo4j Graph update loop using Bolt\n",
    "            try:     \n",
    "                \n",
    "                session = driver.session()\n",
    "\n",
    "                batch_list, batch_msg_consumed = confluent_kafka_consume_batch(consumer, batch_size)\n",
    "                msg_consumed_count += batch_msg_consumed\n",
    "                \n",
    "                # if you want to see what your message batches look like\n",
    "                # df = pd.DataFrame(batch_list)\n",
    "                # filename='test_' + str(msg_consumed_count) + '.csv'\n",
    "                # df.to_csv(path_or_buf= filename)\n",
    "\n",
    "                # using the Bolt implicit transaction\n",
    "                #result = session.run(update_query, {\"batch_list\": batch_list})\n",
    "                \n",
    "                # using the Bolt explicit transaction, recommended for writes\n",
    "                with session.begin_transaction() as tx:\n",
    "                    result = tx.run(update_query, {\"batch_list\": batch_list})\n",
    "                    tx.success = True;\n",
    "                    \n",
    "                    summary = result.consume()\n",
    "                    nodes = summary.counters.nodes_created\n",
    "                    rels = summary.counters.relationships_created\n",
    "\n",
    "                    print(\"%s %s %s %s\" % (\"Messages consumed:\", msg_consumed_count , \"Batch size:\", len(batch_list)), end=\" \")\n",
    "                    print(\"%s %s %s %s\" % (\"Nodes created:\", nodes, \"Rels created:\", rels))\n",
    "                \n",
    "                if msg_consumed_count >= msg_count:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print('*** Got exception',e)\n",
    "                if not isinstance(e, CypherError):\n",
    "                    print('*** Rolling back')\n",
    "                    session.rollback()\n",
    "                else:\n",
    "                    print('*** Not rolling back')\n",
    "\n",
    "            finally:        \n",
    "                session.close()\n",
    "                batch_msg_consumed_count = 0\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            sys.stderr.write('%% Aborted by user\\n')\n",
    "\n",
    "    finally:\n",
    "        consumer_timing = time.time() - consumer_start\n",
    "        consumer.close()    \n",
    "        return consumer_timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Consumer, Update Neo4j Graph in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment: [TopicPartition{topic=neo4j-150K-demo2,partition=0,offset=-1001,error=None}]\n",
      "Messages consumed: 10000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 20000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 30000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 40000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 50000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 60000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 70000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 80000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 90000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 100000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 110000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 120000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 130000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 140000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Messages consumed: 150000 Batch size: 10000 Nodes created: 10000 Rels created: 10000\n",
      "Processed 150000 messsages in 16.80 seconds\n",
      "0.28 MB/s\n",
      "8925.97 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "# run consumer throughput test\n",
    "  \n",
    "consumer_timings['confluent_kafka_consumer'] = confluent_kafka_consumer_performance()\n",
    "\n",
    "calculate_thoughput(consumer_timings['confluent_kafka_consumer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup, in case you want to run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relationships_deleted': 150000, 'nodes_deleted': 150000}\n",
      "*** Done!\n"
     ]
    }
   ],
   "source": [
    "from neo4j.v1 import GraphDatabase, basic_auth, TRUST_ON_FIRST_USE, CypherError\n",
    "\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost\",\n",
    "                          auth=basic_auth(\"neo4j\", \"neo4j\"),\n",
    "                          encrypted=False,\n",
    "                          trust=TRUST_ON_FIRST_USE)\n",
    "\n",
    "cleanup = '''\n",
    "MATCH (n:DailyBoxOffice) DETACH DELETE n\n",
    "'''\n",
    "\n",
    "try:\n",
    "    session = driver.session()\n",
    "    result = session.run(cleanup)\n",
    "    summary = result.consume()\n",
    "    print(summary.counters)\n",
    "    session.close()\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print('*** Got exception',e)\n",
    "    if not isinstance(e, CypherError):\n",
    "        print('*** Rolling back')\n",
    "        session.rollback()\n",
    "    else:\n",
    "        print('*** Not rolling back')\n",
    "\n",
    "finally:        \n",
    "     print('*** Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
